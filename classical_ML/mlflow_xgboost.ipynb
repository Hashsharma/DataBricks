{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c41478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade -Uqqq mlflow>=3.0 xgboost optuna uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f7eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.api.types import CategoricalDtype\n",
    "# from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62e5d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_data(n_samples, n_features, seed = 42, noise_level= 0.3, nonlinear = True) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Generates synthetic regression data with interesting correlations for MLflow and XGBoost demonstrations.\n",
    "\n",
    "    This function creates a DataFrame of continuous features and computes a target variable with nonlinear\n",
    "    relationships and interactions between features. The data is designed to be complex enough to demonstrate\n",
    "    the capabilities of XGBoost, but not so complex that a reasonable model can't be learned.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): Number of samples (rows) to generate.\n",
    "        n_features (int): Number of feature columns.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        noise_level (float, optional): Level of Gaussian noise to add to the target. Defaults to 0.3.\n",
    "        nonlinear (bool, optional): Whether to add nonlinear feature transformations. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.Series]:\n",
    "            - pd.DataFrame: DataFrame containing the synthetic features.\n",
    "            - pd.Series: Series containing the target labels.\n",
    "\n",
    "    Example:\n",
    "        >>> df, target = create_regression_data(n_samples=1000, n_features=10)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # Generate random continuous features\n",
    "    X = rng.uniform(-5, 5, size=(n_samples, n_features))\n",
    "    \n",
    "    # Create feature DataFrame with meaningful names\n",
    "    columns = [f\"feature_{i}\" for i in range(n_features)]\n",
    "    df = pd.DataFrame(X, columns=columns)\n",
    "    \n",
    "    # Generate base target variable with linear relationship to a subset of features\n",
    "    # Use only the first n_features//2 features to create some irrelevant features\n",
    "    weights = rng.uniform(-2, 2, size=n_features//2)\n",
    "    target = np.dot(X[:, :n_features//2], weights)\n",
    "    \n",
    "    # Add some nonlinear transformations if requested\n",
    "    if nonlinear:\n",
    "        # Add square term for first feature\n",
    "        target += 0.5 * X[:, 0]**2\n",
    "        \n",
    "        # Add interaction between the second and third features\n",
    "        if n_features >= 3:\n",
    "            target += 1.5 * X[:, 1] * X[:, 2]\n",
    "        \n",
    "        # Add sine transformation of fourth feature\n",
    "        if n_features >= 4:\n",
    "            target += 2 * np.sin(X[:, 3])\n",
    "        \n",
    "        # Add exponential of fifth feature, scaled down\n",
    "        if n_features >= 5:\n",
    "            target += 0.1 * np.exp(X[:, 4] / 2)\n",
    "            \n",
    "        # Add threshold effect for sixth feature\n",
    "        if n_features >= 6:\n",
    "            target += 3 * (X[:, 5] > 1.5).astype(float)\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noise = rng.normal(0, noise_level * target.std(), size=n_samples)\n",
    "    target += noise\n",
    "    \n",
    "    # Add a few more interesting features to the DataFrame\n",
    "    \n",
    "    # Add a correlated feature (but not used in target calculation)\n",
    "    if n_features >= 7:\n",
    "        df['feature_correlated'] = df['feature_0'] * 0.8 + rng.normal(0, 0.2, size=n_samples)\n",
    "    \n",
    "    # Add a cyclical feature\n",
    "    df['feature_cyclical'] = np.sin(np.linspace(0, 4*np.pi, n_samples))\n",
    "    \n",
    "    # Add a feature with outliers\n",
    "    df['feature_with_outliers'] = rng.normal(0, 1, size=n_samples)\n",
    "    # Add outliers to ~1% of samples\n",
    "    outlier_idx = rng.choice(n_samples, size=n_samples//100, replace=False)\n",
    "    df.loc[outlier_idx, 'feature_with_outliers'] = rng.uniform(10, 15, size=len(outlier_idx))\n",
    "    \n",
    "    return df, pd.Series(target, name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e29d1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Exploratory data analysis (EDA) visualizations\n",
    "# Before training your model, it’s essential to examine your data. Visualizations help you validate that the data is as expected, \n",
    "# spot unexpected anomalies, and drive feature selection. As you move forward with model development, these visualizations \n",
    "# serve as a record of your work that can help with troubleshooting, reproducibility, and collaboration.\n",
    "# You can use MLflow to log visualizations, making your experimentation fully reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab29c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distributions(X: pd.DataFrame, y: pd.Series, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of histograms for each feature in the dataset.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing synthetic features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the distribution plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            sns.histplot(X[feature], ax=ax, kde=True, color='skyblue')\n",
    "            ax.set_title(f'Distribution of {feature}')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Feature Distributions', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_correlation_heatmap(X: pd.DataFrame, y: pd.Series) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a correlation heatmap of all features and the target variable.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the heatmap.\n",
    "    \"\"\"\n",
    "    # Combine features and target into one DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = data.corr()\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Draw the heatmap with a color bar\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap=cmap,\n",
    "                center=0, square=True, linewidths=0.5, ax=ax)\n",
    "    \n",
    "    ax.set_title('Feature Correlation Heatmap', fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_feature_target_relationships(X: pd.DataFrame, y: pd.Series, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of scatter plots showing the relationship between each feature and the target.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the relationship plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            # Scatter plot with regression line\n",
    "            sns.regplot(x=X[feature], y=y, ax=ax, \n",
    "                       scatter_kws={'alpha': 0.5, 'color': 'blue'}, \n",
    "                       line_kws={'color': 'red'})\n",
    "            ax.set_title(f'{feature} vs Target')\n",
    "    \n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Feature vs Target Relationships', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_pairwise_relationships(X: pd.DataFrame, y: pd.Series, features: list[str]) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a pairplot showing relationships between selected features and the target.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        features (List[str]): List of feature names to include in the plot.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the pairplot.\n",
    "    \"\"\"\n",
    "    # Ensure features exist in the DataFrame\n",
    "    valid_features = [f for f in features if f in X.columns]\n",
    "    \n",
    "    if not valid_features:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.text(0.5, 0.5, \"No valid features provided\", ha='center', va='center')\n",
    "        return fig\n",
    "    \n",
    "    # Combine selected features and target\n",
    "    data = X[valid_features].copy()\n",
    "    data['target'] = y\n",
    "    \n",
    "    # Create pairplot\n",
    "    pairgrid = sns.pairplot(data, diag_kind=\"kde\", \n",
    "                          plot_kws={\"alpha\": 0.6, \"s\": 50},\n",
    "                          corner=True)\n",
    "    \n",
    "    pairgrid.fig.suptitle(\"Pairwise Feature Relationships\", y=1.02, fontsize=16)\n",
    "    plt.close(pairgrid.fig)\n",
    "    return pairgrid.fig\n",
    "\n",
    "def plot_boxplots(X: pd.DataFrame, y: pd.Series, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of box plots for each feature, with points colored by target value.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        y (pd.Series): Series containing the target variable.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the box plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    # Create target bins for coloring\n",
    "    y_binned = pd.qcut(y, 3, labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            # Box plot for each feature\n",
    "            sns.boxplot(x=y_binned, y=X[feature], ax=ax)\n",
    "            ax.set_title(f'Distribution of {feature} by Target Range')\n",
    "            ax.set_xlabel('Target Range')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Feature Distributions by Target Range', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_outliers(X: pd.DataFrame, n_cols: int = 3) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a grid of box plots to detect outliers in each feature.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing features.\n",
    "        n_cols (int): Number of columns in the grid layout.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib Figure object containing the outlier plots.\n",
    "    \"\"\"\n",
    "    features = X.columns\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_rows * n_cols > 1 else [axes]\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            # Box plot to detect outliers\n",
    "            sns.boxplot(x=X[feature], ax=ax, color='skyblue')\n",
    "            ax.set_title(f'Outlier Detection for {feature}')\n",
    "            ax.set_xlabel(feature)\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Outlier Detection for Features', y=1.02, fontsize=16)\n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b1369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Standard modeling workflow\n",
    "# The code in the next cell does the following:\n",
    "\n",
    "# Uses the function you created, create_regression_data, to create a dataset.\n",
    "# Uses the visualization functions you created to create EDA plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6d9a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regression dataset\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "X, y = create_regression_data(n_samples=n_samples, n_features=n_features, nonlinear=True)\n",
    "\n",
    "# Create EDA plots\n",
    "dist_plot = plot_feature_distributions(X, y)\n",
    "corr_plot = plot_correlation_heatmap(X, y)\n",
    "scatter_plot = plot_feature_target_relationships(X, y)\n",
    "corr_with_target = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "top_features = corr_with_target.head(4).index.tolist()\n",
    "pairwise_plot = plot_pairwise_relationships(X, y, top_features)\n",
    "outlier_plot = plot_outliers(X)\n",
    "\n",
    "# Configure the XGBoost model\n",
    "reg = xgb.XGBRegressor(\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='rmse',\n",
    ")\n",
    "\n",
    "# Create train/test split to properly evaluate the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7722)\n",
    "\n",
    "# Train the model with evaluation\n",
    "reg.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Generate predictions for residual plot\n",
    "y_pred = reg.predict(X_test)\n",
    "residual_plot = plot_boxplots(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d825a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Log the model using MLflow\n",
    "# When you log a model using MLflow on Databricks, important artifacts and metadata are captured. This ensures that \n",
    "# your model is not only reproducible but also ready for deployment with all necessary dependencies and clear API contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24bf4179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/19 01:50:33 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/12/19 01:50:33 INFO mlflow.store.db.utils: Updating database tables\n",
      "2025/12/19 01:50:33 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/19 01:50:33 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2025/12/19 01:50:33 INFO alembic.runtime.migration: Running upgrade  -> 451aebb31d03, add metric step\n",
      "2025/12/19 01:50:34 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "2025/12/19 01:50:34 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "2025/12/19 01:50:34 INFO alembic.runtime.migration: Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "2025/12/19 01:50:34 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "2025/12/19 01:50:34 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "2025/12/19 01:50:35 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "2025/12/19 01:50:35 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "2025/12/19 01:50:35 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "2025/12/19 01:50:35 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "2025/12/19 01:50:35 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "2025/12/19 01:50:36 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "2025/12/19 01:50:36 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "2025/12/19 01:50:36 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "2025/12/19 01:50:36 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "2025/12/19 01:50:36 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "2025/12/19 01:50:37 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "2025/12/19 01:50:37 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "2025/12/19 01:50:37 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "2025/12/19 01:50:38 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "2025/12/19 01:50:38 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "2025/12/19 01:50:39 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "2025/12/19 01:50:39 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "2025/12/19 01:50:39 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "2025/12/19 01:50:40 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "2025/12/19 01:50:40 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "2025/12/19 01:50:40 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "2025/12/19 01:50:40 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "2025/12/19 01:50:41 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "2025/12/19 01:50:41 INFO alembic.runtime.migration: Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "2025/12/19 01:50:41 INFO alembic.runtime.migration: Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "2025/12/19 01:50:42 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
      "2025/12/19 01:50:42 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
      "2025/12/19 01:50:43 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
      "2025/12/19 01:50:43 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
      "2025/12/19 01:50:44 INFO alembic.runtime.migration: Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
      "2025/12/19 01:50:45 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
      "2025/12/19 01:50:45 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
      "2025/12/19 01:50:46 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
      "2025/12/19 01:50:46 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
      "2025/12/19 01:50:47 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/19 01:50:47 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2025/12/19 01:50:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/12/19 01:50:51 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/12/19 01:50:51 INFO mlflow.store.db.utils: Updating database tables\n",
      "2025/12/19 01:50:51 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/19 01:50:51 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "Successfully registered model 'xgboost_regression_model'.\n",
      "Created version '1' of model 'xgboost_regression_model'.\n",
      "/media/scientist-anand/volume/mr_document/all_venv/databricks_venv/lib/python3.12/site-packages/mlflow/models/evaluation/deprecated.py:9: FutureWarning: The `mlflow.evaluate` API has been deprecated as of MLflow 3.0.0. Please use these new alternatives:\n",
      "\n",
      " - For traditional ML or deep learning models: Use `mlflow.models.evaluate`, which maintains full compatibility with the original `mlflow.evaluate` API.\n",
      "\n",
      " - For LLMs or GenAI applications: Use the new `mlflow.genai.evaluate` API, which offers enhanced features specifically designed for evaluating LLMs and GenAI applications.\n",
      "\n",
      "  warnings.warn(\n",
      "2025/12/19 01:50:54 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-cbae450bdf2445f484e8766b551e6095\n",
      "2025/12/19 01:50:54 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n",
      "2025/12/19 01:50:55 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2025/12/19 01:50:55 WARNING mlflow.models.evaluation.evaluators.shap: SHAP or matplotlib package is not installed, so model explainability insights will not be logged.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: models:/m-cbae450bdf2445f484e8766b551e6095\n",
      "Practice score (Train RMSE): 0.9295\n",
      "Test score (Test RMSE): 7.1552\n"
     ]
    }
   ],
   "source": [
    "# Incorporate MLflow evaluation\n",
    "# Create a copy of the test data and add the actual target values (labels) to it\n",
    "# This creates a complete dataset with both features and actual values for evaluation\n",
    "evaluation_data = X_test.copy()\n",
    "evaluation_data[\"label\"] = y_test\n",
    "\n",
    "# Start a new MLflow tracking run - think of this as opening a new experiment notebook entry\n",
    "with mlflow.start_run() as run:\n",
    "    \n",
    "    # Get the final training error (RMSE) from the last training iteration\n",
    "    # Like checking the final exam score after all study sessions\n",
    "    final_train_rmse = np.array(reg.evals_result()[\"validation_0\"][\"rmse\"])[-1]\n",
    "    \n",
    "    # Get the final testing error (RMSE) from the last validation iteration  \n",
    "    # Like checking the final test score after practice tests\n",
    "    final_test_rmse = np.array(reg.evals_result()[\"validation_1\"][\"rmse\"])[-1]\n",
    "    \n",
    "    # Extract all the model settings/parameters (like learning rate, depth, etc.)\n",
    "    # This is like writing down the recipe you used to bake a cake\n",
    "    feature_map = {key: value for key, value in reg.get_xgb_params().items() if value is not None}\n",
    "\n",
    "    # Create a \"signature\" that defines what data goes in and what comes out\n",
    "    # This is like creating a label that says \"Input: flour, sugar, eggs → Output: cake\"\n",
    "    signature = infer_signature(X, reg.predict(X))\n",
    "\n",
    "    # Save all the model parameters to MLflow for future reference\n",
    "    # Like storing the recipe card in your kitchen drawer\n",
    "    mlflow.log_params(feature_map)\n",
    "    \n",
    "    # For MLflow 3.0+, use sklearn flavor for XGBoost scikit-learn API models\n",
    "    # First, check if our model is built using sklearn style\n",
    "    if hasattr(reg, '_estimator_type'):\n",
    "        # If yes, save it as a sklearn-style model\n",
    "        # Like storing a recipe in your \"baking recipes\" notebook\n",
    "        model_info = mlflow.sklearn.log_model(\n",
    "            sk_model=reg,                    # The actual model (our trained \"brain\")\n",
    "            artifact_path=\"model\",           # Folder name to save it in\n",
    "            input_example=X.iloc[[0]],       # Show an example of what input looks like\n",
    "            signature=signature,             # The input/output label we created\n",
    "            registered_model_name=\"xgboost_regression_model\"  # Official name in model registry\n",
    "        )\n",
    "    else:\n",
    "        # If not sklearn-style, save it as a pure XGBoost model\n",
    "        # Like storing a recipe in your \"special techniques\" notebook\n",
    "        model_info = mlflow.xgboost.log_model(\n",
    "            xgb_model=reg.get_booster(),     # Get the core engine of XGBoost\n",
    "            artifact_path=\"model\",           # Folder name to save it in\n",
    "            input_example=X.iloc[[0]],       # Show an example input\n",
    "            signature=signature,             # The input/output label\n",
    "            registered_model_name=\"xgboost_regression_model\"  # Official name\n",
    "        )\n",
    "\n",
    "    # Save the training performance score to MLflow\n",
    "    # Like writing \"Practice test score: 95%\" in your study log\n",
    "    mlflow.log_metric(\"train_rmse\", final_train_rmse)\n",
    "    \n",
    "    # Save the testing performance score to MLflow  \n",
    "    # Like writing \"Final exam score: 92%\" in your study log\n",
    "    mlflow.log_metric(\"test_rmse\", final_test_rmse)\n",
    "    \n",
    "    # Save all the visualization charts we created earlier\n",
    "    # Like putting photos of your cooking process in the recipe book\n",
    "    mlflow.log_figure(dist_plot, \"feature_distributions.png\")           # How ingredients vary\n",
    "    mlflow.log_figure(corr_plot, \"correlation_heatmap.png\")             # How ingredients relate\n",
    "    mlflow.log_figure(scatter_plot, \"feature_target_relationships.png\") # How ingredients affect outcome\n",
    "    mlflow.log_figure(pairwise_plot, \"pairwise_relationships.png\")      # Ingredient pair relationships\n",
    "    mlflow.log_figure(outlier_plot, \"outlier_detection.png\")            # Strange/unusual ingredients\n",
    "    mlflow.log_figure(residual_plot, \"feature_boxplots_by_target.png\")  # Ingredient patterns by outcome\n",
    "        \n",
    "    # Create and save a chart showing which features are most important\n",
    "    # Like creating a chart showing \"flour is 40% important, sugar 30%, eggs 30%\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))      # Create empty canvas for chart\n",
    "    xgb.plot_importance(reg, ax=ax, importance_type='gain')  # Draw importance chart\n",
    "    plt.title('Feature Importance')              # Add title\n",
    "    plt.tight_layout()                           # Make it look neat\n",
    "    plt.close(fig)                               # Close the drawing tool\n",
    "    mlflow.log_figure(fig, \"feature_importance.png\")  # Save to MLflow\n",
    "\n",
    "    # Run a comprehensive evaluation using the saved model\n",
    "    # Like having a food critic taste and score your cake using the stored recipe\n",
    "    result = mlflow.evaluate(\n",
    "        model=model_info.model_uri,      # Where the saved model is stored\n",
    "        data=evaluation_data,            # Test data with actual answers\n",
    "        targets=\"label\",                 # Which column has the actual answers\n",
    "        model_type=\"regressor\",          # What type of model this is\n",
    "        evaluators=[\"default\"]           # Use standard evaluation methods\n",
    "    )\n",
    "    \n",
    "    # Print helpful information for the user\n",
    "    print(f\"Model saved at: {model_info.model_uri}\")        # Where to find it later\n",
    "    print(f\"Practice score (Train RMSE): {final_train_rmse:.4f}\")  # Training performance\n",
    "    print(f\"Test score (Test RMSE): {final_test_rmse:.4f}\")        # Testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d6529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databricks_venv",
   "language": "python",
   "name": "databricks_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
